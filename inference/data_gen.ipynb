{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from polygon import RESTClient\n",
    "from datetime import datetime, timedelta\n",
    "import pandas_market_calendars as mcal\n",
    "import polygon\n",
    "import os, pickle\n",
    "from scipy.optimize import minimize\n",
    "from nelson_siegel_svensson.calibrate import calibrate_nss_ols\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from multiprocess import Pool\n",
    "from joblib import Parallel, delayed\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_diff = lambda date1, date2 : (datetime.strptime(date1, '%Y-%m-%d') - datetime.strptime(date2, '%Y-%m-%d')).days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"/Users/brad/mlprojects/guidelight/guidelight-api/.env\")\n",
    "polygon_token = os.getenv(\"POLYGON_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.exists(\"data.pkl\"):\n",
    "    indices = [(contract.ticker, contract.expiration_date, contract.strike_price) for contract in all_contracts]\n",
    "    data = {}\n",
    "    for index in indices:\n",
    "        ticker, expiration_date, strike_price = index\n",
    "        current_date = datetime.strptime(expiration_date, \"%Y-%m-%d\")\n",
    "        past_date = current_date - timedelta(days=14)\n",
    "\n",
    "        # get key value data for each agg\n",
    "\n",
    "        a = [vars(agg) for agg in client.get_aggs(ticker, 1, 'day', past_date, current_date)]\n",
    "        data[index] = a\n",
    "\n",
    "\n",
    "    pickle.dump(data, open(\"data.pkl\", \"wb\"))\n",
    "\n",
    "else:\n",
    "    data = pickle.load(open(\"data.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_worker(agg):\n",
    "    return vars(agg)\n",
    "\n",
    "def generate_option_aggs(ticker, token=polygon_token):\n",
    "    client = RESTClient(api_key=token)\n",
    "    if not os.path.exists(f\"cache/{ticker}_contracts.pkl\"):\n",
    "        reqs = client.list_options_contracts(ticker,as_of=\"2024-04-16\", expired=True, expiration_date_gt=\"2023-04-16\")\n",
    "        all_contracts = list(reqs)\n",
    "        pickle.dump(all_contracts, open(f\"cache/{ticker}_contracts.pkl\", \"wb\"))\n",
    "    else:\n",
    "        all_contracts = pickle.load(open(f\"cache/{ticker}_contracts.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "    indices = [(contract.ticker, contract.expiration_date, contract.strike_price) for contract in all_contracts]\n",
    "    data = {}\n",
    "    if not os.path.exists(\"data.pkl\"):\n",
    "        for index in indices:\n",
    "            ticker, expiration_date, strike_price = index\n",
    "            current_date = datetime.strptime(expiration_date, \"%Y-%m-%d\")\n",
    "            past_date = current_date - timedelta(days=14)\n",
    "\n",
    "            # Fetch aggregates for each contract within the date range\n",
    "            aggs = client.get_aggs(ticker, 1, 'day', past_date, current_date)\n",
    "            \n",
    "            # Using Pool for asynchronous map\n",
    "            with Pool(10) as p:\n",
    "                async_result = p.map_async(get_agg_worker, aggs)\n",
    "                p.close()  # No more tasks will be submitted, safe to close the pool\n",
    "                p.join()  # Wait for all worker processes to finish\n",
    "                \n",
    "                # Collect results\n",
    "                results = async_result.get()\n",
    "                data[index] = results\n",
    "    else:\n",
    "        data = pickle.load(open(\"data.pkl\", \"rb\"))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_option_ticker(underlying_ticker:str, data):\n",
    "# Flatten the data while preserving the option ticker and expiration date\n",
    "\tflattened_data = []\n",
    "\tfor (ticker, expiration, strike_price), entries in data.items():\n",
    "\t\tfor entry in entries:\n",
    "\t\t\tentry.update({\n",
    "\t\t\t\t\"ticker\": ticker,\n",
    "\t\t\t\t\"expiration_date\": expiration,\n",
    "\t\t\t\t\"strike_price\": strike_price\n",
    "\t\t\t})\n",
    "\t\t\tflattened_data.append(entry)\n",
    "\n",
    "\t# Create a DataFrame\n",
    "\tdf = pd.DataFrame(flattened_data)\n",
    "\n",
    "\t# Set a MultiIndex using the ticker, expiration date, and trading date\n",
    "\tdf.set_index(['ticker', \"strike_price\", 'expiration_date'], inplace=True)\n",
    "\n",
    "\t# get by ticker\n",
    "\t# 1681099200000\n",
    "\tdf['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms').dt.strftime(\"%Y-%m-%d\")\n",
    "\t# df.index = df.index.set_levels(pd.to_datetime(df.index.get_level_values('timestamp'), unit='ms').strftime('%Y-%m-%d %H:%M:%S'), level='timestamp')\n",
    "\tdf.to_csv(f\"options_contracts/{underlying_ticker.upper()}.csv\", index_label=['ticker', \"strike_price\", 'expiration_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data while preserving the option ticker and expiration date\n",
    "flattened_data = []\n",
    "for (ticker, expiration, strike_price), entries in data.items():\n",
    "    for entry in entries:\n",
    "        entry.update({\n",
    "            \"ticker\": ticker,\n",
    "            \"expiration_date\": expiration,\n",
    "            \"strike_price\": strike_price\n",
    "        })\n",
    "        flattened_data.append(entry)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(flattened_data)\n",
    "\n",
    "# Set a MultiIndex using the ticker, expiration date, and trading date\n",
    "df.set_index(['ticker', \"strike_price\", 'expiration_date'], inplace=True)\n",
    "\n",
    "# get by ticker\n",
    "# 1681099200000\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms').dt.strftime(\"%Y-%m-%d\")\n",
    "# df.index = df.index.set_levels(pd.to_datetime(df.index.get_level_values('timestamp'), unit='ms').strftime('%Y-%m-%d %H:%M:%S'), level='timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_option_data(underlying_ticker:str, date:str):\n",
    "\tif os.path.exists(f\"options_contracts/{underlying_ticker}-{date}.csv\"):\n",
    "\t\treturn pd.read_csv(f\"options_contracts/{underlying_ticker}-{date}.csv\")\n",
    "\n",
    "\tdf = pd.read_csv(f'options_contracts/{underlying_ticker}.csv', index_col=[0, 1, 2])\n",
    "\toption_contracts = df.loc[df['timestamp'] == date]\n",
    "\toption_contracts.reset_index(inplace=True)\n",
    "\t# print(option_contracts)\n",
    "\tcolnames = [\"ticker\", \"maturity\", \"Weight\", 'price', 'days since last trade', 'strike', 'S']\n",
    "\tvolsurface = pd.DataFrame(columns=colnames)\n",
    "\n",
    "\tfor ticker in option_contracts['ticker'].unique():\n",
    "\t\tagg_series= df.loc[(ticker, slice(None), slice(None))]\n",
    "\t\ti = np.where(agg_series['timestamp'].values == date)[0][0]\n",
    "\t# agg_series\n",
    "\t\tif i <= 0:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tdiff = datetime_diff(agg_series['timestamp'].iloc[i], agg_series['timestamp'].iloc[i-1])\n",
    "\t\tif diff <= 3:\n",
    "\t\t\texpiration_date = agg_series.index.get_level_values(1).unique()[0]\n",
    "\t\t\ttime_to_maturity =datetime_diff(expiration_date, date) \n",
    "\t\t\trow = pd.DataFrame({\n",
    "\t\t\t\t'ticker': ticker,\n",
    "\t\t\t\t'maturity': time_to_maturity/365 if time_to_maturity else 6.5/(24 * 365),\n",
    "\t\t\t\t'price': agg_series[\"vwap\"].values[i],\n",
    "\t\t\t\t'Weight': agg_series[\"volume\"].values[i] / agg_series[\"volume\"].sum(),\n",
    "\t\t\t\t'days since last trade': diff,\n",
    "\t\t\t\t'strike': agg_series.index.get_level_values(0).unique()[0],\n",
    "\t\t\t\t'S': agg_series['open'].values[i]\n",
    "\t\t\t}, columns=colnames, index=[0])\n",
    "\n",
    "\t\t\tvolsurface = pd.concat([volsurface, row], ignore_index=True)\n",
    "\t\t\t\n",
    "\n",
    "\t\n",
    "\tvolsurface.to_csv( os.path.join(os.getcwd(), f\"options_data/{underlying_ticker}-{date}.csv\"))\n",
    "\treturn volsurface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def worker(date, underlying_ticker):\n",
    "    return daily_option_data(underlying_ticker, date)\n",
    "\n",
    "\n",
    "def process_multiple_days(underlying_ticker, start_date, end_date):\n",
    "    # Generate list of dates\n",
    "    dates = mcal.get_calendar(\"NYSE\").valid_days(start_date=start_date, end_date=end_date)\n",
    "    \n",
    "    # Define a helper to wrap your existing function for use with starmap\n",
    "    \n",
    "\n",
    "    # Setup multiprocessing pool\n",
    "    # with Pool() as pool:\n",
    "    #     pool.starmap(worker, [(underlying_ticker, date) for date in dates])\n",
    "\n",
    "    # print(\"Data processing complete for all specified dates.\")\n",
    "    dataset = [daily_option_data(underlying_ticker, timestamp.date().strftime(\"%Y-%m-%d\")) for timestamp in dates]\n",
    "\n",
    "    # return dataset\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yield_rates = pd.read_csv(\"five-year-rates.csv\")\n",
    "# yield_maturities = np.array([1/12, 2/12, 3/12, 4/12, 6/12, 1, 2, 3, 5, 7, 10, 20, 30])\n",
    "# d = datetime.strftime(datetime.strptime(\"2023-04-18\", \"%Y-%m-%d\"), \"%m/%d/%Y\")\n",
    "# yields = yield_rates.loc[yield_rates[\"Date\"]==d].values[:,1:].astype(np.float128).reshape(-1)\n",
    "# scaler = MinMaxScaler()\n",
    "# yields_normalized = scaler.fit_transform(yields.reshape(-1, 1)).flatten()\n",
    "# calibrate_nss_ols(yield_maturities, yields_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_voilatilities = torch.empty(len(daily_aggs))\n",
    "# for i, day in enumerate(daily_aggs):\n",
    "# \thist_voilatilities[i] = estimate_historical_volatility(day['close'].values)\n",
    "\n",
    "# # calculate the historical volatility for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from heston_param import *\n",
    "# hist_voilatilities = pickle.load(open(\"hist_voilatilities.pkl\", \"rb\"))\n",
    "# daily_aggs = pickle.load(open(\"daily_aggs.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "# \ttry:\n",
    "# \t\tprint(calibrate_daily_parameters(hist_voilatilities[0], 0.1, daily_aggs[0][\"close\"].values, 0.0237, daily_aggs[0][\"close\"].values.shape[0], 50))\n",
    "# \t\tbreak\n",
    "# \texcept RuntimeError:\n",
    "# \t\tcontinue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def single_day_calibration(args):\n",
    "# \t\ti, hist_volatilities, daily_agg = args\n",
    "# \t\twhile True:\n",
    "# \t\t\ttry:\n",
    "# \t\t\t\tparams = calibrate_daily_parameters(hist_volatilities[i], 0.1, day[\"close\"].values, 0.0237, day[\"close\"].values.shape[0], 300)\n",
    "\t\t\t\t\n",
    "# \t\t\t\treturn i, params\n",
    "# \t\t\texcept RuntimeError:\n",
    "# \t\t\t\tcontinue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(daily_aggs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Monte Carlo:\n",
    "Runtime: 1.15 Hour. Suboptimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_params = np.empty((len(daily_aggs), 5))\n",
    "# for i, day in enumerate((daily_aggs[:20])):\n",
    "# \twhile True:\n",
    "# \t\ttry:\n",
    "# \t\t\tdaily_params[i] = calibrate_daily_parameters(hist_voilatilities[i], 0.1, daily_aggs[i][\"close\"].values, 0.0237, daily_aggs[i][\"close\"].values.shape[0], 50)\n",
    "# \t\t\tif (i + 1) % 10 == 0:\n",
    "# \t\t\t\t\trate =  100 *  np.round((i + 1) /len(daily_params[:20]), 2)\n",
    "# \t\t\t\t\tprint(f\"{rate}% completed.\")\n",
    "\t\t\t\n",
    "# \t\t\tbreak\n",
    "# \t\texcept RuntimeError:\n",
    "# \t\t\tcontinue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calibrate_worker(args):\n",
    "# \tsingle_day_calibration(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_params = pickle.load(open(\"parameters.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runtime is 1.5 hours\n",
    "# daily_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocess import Pool\n",
    "\n",
    "# def calibrate_parameters_multiprocessing(daily_aggs, hist_volatilities):\n",
    "#     # Prepare arguments for each task\n",
    "#     tasks = [(i, hist_volatilities[i], daily_aggs[i]) for i in range(len(daily_aggs))]\n",
    "\n",
    "#     # Number of processes, could be set to the number of CPUs or cores\n",
    "#     num_processes = 4\n",
    "\n",
    "#     # Create a multiprocessing pool and map tasks to worker function\n",
    "#     with Pool(processes=num_processes) as pool:\n",
    "#         results = pool.map(calibrate_worker, tasks)\n",
    "\n",
    "#     # Process results\n",
    "#     daily_params = np.empty((len(daily_aggs[:20]), 5))\n",
    "#     for index, params in results:\n",
    "#         if params is not None:\n",
    "#             daily_params[index] = params\n",
    "#         else:\n",
    "#             print(f\"Calibration failed for index {index}\")\n",
    "\n",
    "#     return daily_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate_parameters_multiprocessing(daily_aggs[:20], hist_voilatilities[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Simulation using FFT for the characteristic equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@alexander.tsoskounoglou/pricing-options-with-fourier-series-p3-the-heston-model-d157369a217a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heston_char(u, params):\n",
    "    kappa, theta, zeta, rho, v0, r, q, T, S0 = params \n",
    "    t0 = 0.0 ;  q = 0.0\n",
    "    m = np.log(S0) + (r - q)*(T-t0)\n",
    "    D = np.sqrt((rho*zeta*1j*u - kappa)**2 + zeta**2*(1j*u + u**2))\n",
    "    C = (kappa - rho*zeta*1j*u - D) / (kappa - rho*zeta*1j*u + D)\n",
    "    beta = ((kappa - rho*zeta*1j*u - D)*(1-np.exp(-D*(T-t0)))) / (zeta**2*(1-C*np.exp(-D*(T-t0))))\n",
    "    alpha = ((kappa*theta)/(zeta**2))*((kappa - rho*zeta*1j*u - D)*(T-t0) - 2*np.log((1-C*np.exp(-D*(T-t0))/(1-C))))\n",
    "    return np.exp(1j*u*m + alpha + beta*v0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sqrt, exp, pi, cos, sin, log, abs\n",
    "from numba import njit\n",
    "\n",
    "@njit(parallel=True)\n",
    "def Fourier_Heston_Put(S0, K, T, r, \n",
    "                  # Heston Model Paramters\n",
    "                  kappa, # Speed of the mean reversion \n",
    "                  theta, # Long term mean\n",
    "                  rho,   # correlation between 2 random variables\n",
    "                  zeta,  # Volatility of volatility\n",
    "                  v0,    # Initial volatility \n",
    "                  opt_type,\n",
    "                  N = 1_012,\n",
    "                  z = 24\n",
    "                  ):\n",
    "\n",
    "  def heston_char(u): \n",
    "    t0 = 0.0 ;  q = 0.0\n",
    "    m = log(S0) + (r - q)*(T-t0)\n",
    "    D = sqrt((rho*zeta*1j*u - kappa)**2 + zeta**2*(1j*u + u**2))\n",
    "    C = (kappa - rho*zeta*1j*u - D) / (kappa - rho*zeta*1j*u + D)\n",
    "    beta = ((kappa - rho*zeta*1j*u - D)*(1-exp(-D*(T-t0)))) / (zeta**2*(1-C*exp(-D*(T-t0))))\n",
    "    alpha = ((kappa*theta)/(zeta**2))*((kappa - rho*zeta*1j*u - D)*(T-t0) - 2*log((1-C*exp(-D*(T-t0))/(1-C))))\n",
    "    return exp(1j*u*m + alpha + beta*v0)\n",
    "  \n",
    "  # # Parameters for the Function to make sure the approximations are correct.\n",
    "  c1 = log(S0) + r*T - .5*theta*T\n",
    "  c2 = theta/(8*kappa**3)*(-zeta**2*exp(-2*kappa*T) + 4*zeta*exp(-kappa*T)*(zeta-2*kappa*rho) \n",
    "        + 2*kappa*T*(4*kappa**2 + zeta**2 - 4*kappa*zeta*rho) + zeta*(8*kappa*rho - 3*zeta))\n",
    "  a = c1 - z*sqrt(abs(c2))\n",
    "  b = c1 + z*sqrt(abs(c2))\n",
    "  \n",
    "  h       = lambda n : (n*pi) / (b-a) \n",
    "  g_n     = lambda n : (exp(a) - (K/h(n))*sin(h(n)*(a - log(K))) - K*cos(h(n)*(a - log(K)))) / (1 + h(n)**2)\n",
    "  g0      = K*(log(K) - a - 1) + exp(a)\n",
    "  \n",
    "  F = g0 \n",
    "  for n in range(1, N+1):\n",
    "    h_n = h(n)\n",
    "    F += 2*heston_char(h_n) * exp(-1j*a*h_n) * g_n(n)\n",
    "\n",
    "  F = exp(-r*T)/(b-a) * np.real(F)\n",
    "  F = F if opt_type == 'p' else F + S0 - K*exp(-r*T)\n",
    "  return F if F > 0 else 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0      = 100.      # initial asset price\n",
    "K       = 50.       # strike\n",
    "r       = 0.03      # risk free rate\n",
    "T       = 1/365     # time to maturity\n",
    "\n",
    "v0=0.4173 ; kappa=0.4352 ; theta=0.2982 ; zeta=1.3856 ; rho=-0.0304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vollib_vectorized\n",
    "\n",
    "price = 0.10 ; S = 95 ; K = 100 ; t = .2 ; r = .2 ; flag = 'c'\n",
    "\n",
    "def implied_volatility(price, S, K, t, r, flag):\n",
    "  return py_vollib_vectorized.vectorized_implied_volatility(\n",
    "    price, S, K, t, r, flag, q=0.0, on_error='ignore', model='black_scholes_merton',return_as='numpy') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pyFFTW\n",
    "import pyfftw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import exp, pi, log, sqrt\n",
    "# from  numpy.fft import fft\n",
    "import pyfftw\n",
    "@njit(parallel=True)\n",
    "def heston_fft2(S0, K, T, r, kappa, theta, rho, zeta, v0, opt_type, N=1024, alpha=1.5):\n",
    "    eta = 0.25  # Grid spacing for the integration variable\n",
    "    lambda_u = 2 * pi / (N * eta)  # Grid spacing for log strike\n",
    "\n",
    "    # Adjustments for the damping factor\n",
    "    alpha = alpha  # Damping factor, typically 1 or 1.5\n",
    "\n",
    "    # Characteristic function for the Heston model as before\n",
    "    def heston_char(u):\n",
    "        t0 = 0.0 ; q = 0.0\n",
    "        m = log(S0) + (r - q) * (T - t0)\n",
    "        D = sqrt((rho * zeta * 1j * u - kappa) ** 2 + zeta ** 2 * (1j * u + u ** 2))\n",
    "        C = (kappa - rho * zeta * 1j * u - D) / (kappa - rho * zeta * 1j * u + D)\n",
    "        beta = ((kappa - rho * zeta * 1j * u - D) * (1 - exp(-D * (T - t0)))) / (zeta ** 2 * (1 - C * exp(-D * (T - t0))))\n",
    "        alpha = ((kappa * theta) / (zeta ** 2)) * ((kappa - rho * zeta * 1j * u - D) * (T - t0) - 2 * log((1 - C * exp(-D * (T - t0))) / (1 - C)))\n",
    "        return exp(1j * u * m + alpha + beta * v0)\n",
    "\n",
    "    # Array of discretized u values (integration variable)\n",
    "    u = np.arange(1, N) * eta\n",
    "    u = np.hstack((np.array([0.000001]), u))  # Avoid division by zero in calculations\n",
    "\n",
    "    # Weights for the integration\n",
    "    weights = np.ones(N)\n",
    "    weights[0] = 0.5  # Trapezoidal rule: first weight is 0.5\n",
    "    weights = weights * eta\n",
    "\n",
    "    # Damping factor applied to characteristic function\n",
    "    adjusted_char_fn = exp(-r * T) * (heston_char(u - (alpha + 1) * 1j) / (alpha ** 2 + alpha - u ** 2 + 1j * (2 * alpha + 1) * u))\n",
    "\n",
    "    # FFT calculation\n",
    "    fft_object = pyfftw.builders.rfft(adjusted_char_fn.real * weights, threads=4)\n",
    "    fft_values = fft_object()\n",
    "    fft_values = fft_values[:N // 2]  # Only need the first half of the FFT output\n",
    "\n",
    "    # Calculate strike prices corresponding to FFT output\n",
    "    strikes = S0 * exp(-lambda_u * np.arange(N // 2))\n",
    "\n",
    "    # Option prices\n",
    "    prices = np.exp(-alpha * np.log(strikes)) / pi * fft_values.real\n",
    "\n",
    "    # Find the index of the strike closest to K\n",
    "    index = np.argmin(np.abs(strikes - K))\n",
    "    price = prices[index]\n",
    "    return price if opt_type == 'p' else price + S0 - K * exp(-r * T)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nelson_siegel_svensson.calibrate import calibrate_nss_ols\n",
    "# yield_maturities = np.array([1/12, 2/12, 3/12, 4/12, 6/12, 1, 2, 3, 5, 7, 10, 20, 30])\n",
    "# # yields  = np.array([5.30,5.39,5.50,5.50,5.44,5.11,4.33,3.98,3.70,3.66,3.61,3.98,3.84])\n",
    "# # get the first row of the yield rates\n",
    "# yields = yield_rates.iloc[0].values[1:].astype(np.float64)\n",
    "# curve_fit, status = calibrate_nss_ols(yield_maturities,yields)\n",
    "# # yields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve_fit, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_implied_volatility(price, S, K, t, r, flag):\n",
    "    return py_vollib_vectorized.vectorized_implied_volatility(\n",
    "        price, S, K, t, r, flag, q=0.0, on_error='ignore', model='black_scholes_merton',return_as='numpy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_debug_info(v0, kappa, theta, zeta, rho, wmae, idx, zeros, rmse):\n",
    "    print(\n",
    "        f\">>v0={v0:.4f}; kappa={kappa:.4f}; theta={theta:.4f}; \"\n",
    "        f\"zeta={zeta:.4f}; rho={rho:7.4f} | WMAE(IV): {wmae:.5e} | \"\n",
    "        f\"Nulls: {idx.sum()}/{idx.shape[0]} | Zeros: {zeros}/{idx.shape[0]} | \"\n",
    "        f\"WRMSE(IV): {rmse:.5e}\"\n",
    "    )\n",
    "def SqErr(x, volSurface, _S, _K, _T, _r, _IV, _Weight):\n",
    "    \n",
    "    v0, kappa, theta, zeta, rho = x\n",
    "\n",
    "    # Calculate prices using Heston Model\n",
    "    Price_Heston = get_resutls_array_Heston(\n",
    "        volSurface, v0, kappa, theta, zeta, rho, N=1_012, z=24\n",
    "    )\n",
    "    \n",
    "    # Calculate implied volatilities\n",
    "    IV_Heston = get_implied_volatility(\n",
    "        price=Price_Heston, S=_S, K=_K, t=_T, r=_r, flag='p'\n",
    "    )\n",
    "    \n",
    "    # Handle undefined IV calculations\n",
    "    diff = IV_Heston - _IV\n",
    "    idx = np.isnan(diff) | np.isinf(diff)\n",
    "    diff[idx] = 0 - _IV[idx]\n",
    "    IV_Heston[idx] = 0\n",
    "    diff = np.nan_to_num(diff, 0)\n",
    "    # Calculate RMSE\n",
    "    rmse = sqrt(np.mean((diff * 100) ** 2 * _Weight))\n",
    "    \n",
    "    # Debugging info\n",
    "    zeros = int(np.where(IV_Heston == 0, 1, 0).sum())\n",
    "    wmae  = np.mean(np.abs(diff * 100) * _Weight)\n",
    "    print_debug_info(v0, kappa, theta, zeta, rho, wmae, idx, zeros, rmse)\n",
    "    return rmse\n",
    "def get_error_Heston(volSurface, v0, kappa, theta, zeta, rho):\n",
    "    \"\"\"Calculates the error between the Heston model and the market prices.\n",
    "    Arguments:\n",
    "        volSurface: DataFrame with the market prices.\n",
    "        v0: Initial variance.\n",
    "        kappa: Mean reversion speed.\n",
    "        theta: Long-run variance.\n",
    "        zeta: Volatility of volatility.\n",
    "        rho: Correlation between the variance and the asset.\n",
    "    \"\"\"\n",
    "    error = 0\n",
    "    for _, row in volSurface.iterrows():\n",
    "        P = row['price']\n",
    "        HP = Fourier_Heston_Put(S0=row['S'], K=row['strike'], v0=v0, kappa=kappa, theta=theta, zeta=zeta, rho=rho, T=row['maturity'], r=row['rate'], N=2048)\n",
    "        error += (P - HP)**2\n",
    "\n",
    "    return error / volSurface.shape[0]\n",
    "\n",
    "def get_resutls_array_Heston(volSurface, v0, kappa, theta, zeta, rho, N=10_000, z=64):\n",
    "    # Initialize the results array\n",
    "    results = -np.ones(volSurface.shape[0])\n",
    "    # reset the index of the options dataframe\n",
    "    volSurface.index = np.arange(0, volSurface.shape[0])\n",
    "    # loop through the rows of the options dataframe and run the Fourier_Heston_Put function\n",
    "   \n",
    "    for idx, row in volSurface.iterrows():\n",
    "        results[idx] = Fourier_Heston_Put(S0=int(row['S']), K=int(row['strike']), v0=v0, kappa=kappa, theta=theta, zeta=zeta, rho=rho, T=row['maturity'], r=row['rate'], N=N, opt_type='p',z=z)\n",
    "    return results\n",
    "\n",
    "def get_resutls_df_Heston(volSurface, v0, kappa, theta, zeta, rho, N=2048, z=100):\n",
    "    observed = volSurface.copy(deep=True)\n",
    "    heston = volSurface.copy(deep=True)\n",
    "    observed['source'] = 'Observed'\n",
    "    heston['source'] = 'Heston Model'\n",
    "\n",
    "    heston_prices = [] \n",
    "    implied_volatilities = []\n",
    "    for _, row in volSurface.iterrows():\n",
    "        heston_price = Fourier_Heston_Put(S0=row['S'], K=row['strike'], v0=v0, kappa=kappa, theta=theta, zeta=zeta, rho=rho, T=row['maturity'], r=row['rate'], N=N, opt_type='p', z=z)\n",
    "        heston_prices.append(heston_price)\n",
    "        # np.array(... , ndmin=1) So the type of the input is compatible with what numba expects\n",
    "        maturity  = np.array(row['maturity'],ndmin=1)\n",
    "        observed_price  = np.array(heston_price,ndmin=1)\n",
    "        S0 = np.array(row['S'],ndmin=1)\n",
    "        K  = np.array(row['strike'],ndmin=1)\n",
    "        r  = np.array(row['rate'],ndmin=1)\n",
    "        implied_volatility = get_implied_volatility(price=observed_price, S=S0, K=K, t=maturity, r=r, flag=option_type)\n",
    "        implied_volatilities.append(implied_volatility[0])\n",
    "\n",
    "    heston['price'] = heston_prices\n",
    "    heston['IV']    = implied_volatilities\n",
    "\n",
    "    return pd.concat([observed, heston])\n",
    "\n",
    "def get_error_df_Heston(volSurface, v0, kappa, theta, zeta, rho, diff='Price', error='Error', weighted=True, N=10_000, z=64):\n",
    "    if   error == 'Error':          _name = f'Weighted Error {diff}'             if weighted else f'Error {diff}'\n",
    "    elif error == 'Perc Error':     _name = f'Weighted Persentage Error {diff}'  if weighted else f'Persentage Error {diff}'\n",
    "    elif error == 'Squared Error':  _name = f'Weighted Squared Error {diff}'     if weighted else f'Squared Error {diff}'\n",
    "    else: raise Exception(\"Error: variable 'error' is not defined correctly\")\n",
    "    \n",
    "    results_df = {'strike':[], 'maturity':[], _name:[], 'Opt. Type':[], 'Weight':[]}\n",
    "\n",
    "    for _, row in volSurface.copy(deep=True).iterrows():\n",
    "        _P = Fourier_Heston_Put(S0=row['S'], K=row['strike'], v0=v0, kappa=kappa, theta=theta, zeta=zeta, rho=rho, T=row['maturity'], r=row['rate'], N=N, z=z, opt_type=row['Type'])\n",
    "        # np.array(... , ndmin=1) So the type of the input is compatible with what numba expects\n",
    "        _T  = np.array(row['maturity'],ndmin=1)\n",
    "        _C  = np.array(_P,ndmin=1)\n",
    "        _P  = np.array(row['price'],ndmin=1)\n",
    "        _S0 = np.array(row['S'],ndmin=1)\n",
    "        _K  = np.array(row['strike'],ndmin=1)\n",
    "        _r  = np.array(row['rate'],ndmin=1)\n",
    "\n",
    "        _IV  = get_implied_volatility(price=_C, S=_S0, K=_K, t=_T, r=_r, flag='p')\n",
    "        _IV2 = get_implied_volatility(price=row['price'], S=_S0, K=_K, t=_T, r=_r, flag='p')\n",
    "\n",
    "        if error    == 'Error':\n",
    "            if diff == 'IV':  _error  = (_IV - _IV2) *                (row['Weight'] if weighted else 1)\n",
    "            else           :  _error  = (_C - _P) *                   (row['Weight'] if weighted else 1)\n",
    "        elif error  == 'Perc Error':\n",
    "            if diff == 'IV':  _error  = ((_IV - _IV2)/_IV2) * 100 *   (row['Weight'] if weighted else 1)\n",
    "            else           :  _error  = ((_C - _P)/_P) * 100 *        (row['Weight'] if weighted else 1)\n",
    "        elif error  == 'Squared Error':\n",
    "            if diff == 'IV':  _error  = (_IV - _IV2)**2 *             (row['Weight'] if weighted else 1)\n",
    "            else           :  _error  = (_C - _P)**2 *                (row['Weight'] if weighted else 1)\n",
    "\n",
    "        results_df[_name].append(_error[0])\n",
    "        results_df['maturity'].append(_T[0])\n",
    "        results_df['strike'].append(_K[0])\n",
    "        results_df['Weight'].append(row['Weight']*10)\n",
    "\n",
    "    return pd.DataFrame(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vollib_vectorized\n",
    "def heston_volSurface(cleaned_df, yields):\n",
    " \n",
    "    volSurface = cleaned_df.drop(columns=['days since last trade', 'ticker'])\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    def implied_volatility(price, S, K, t, r, flag):\n",
    "\n",
    "        return py_vollib_vectorized.vectorized_implied_volatility(\n",
    "            price, S, K, t, r, flag, q=0.0, on_error='ignore', model='black_scholes_merton',return_as='numpy')\n",
    "\n",
    "    yield_maturities = np.array([1/12, 2/12, 3/12, 4/12, 6/12, 1, 2, 3, 5, 7, 10, 20, 30])\n",
    "    # yields  = np.array([5.30,5.39,5.50,5.50,5.44,5.11,4.33,3.98,3.70,3.66,3.61,3.98,3.84])\n",
    "    # get the first row of the yield rates\n",
    "    yield_rates = pd.read_csv(\"five-year-rates.csv\")\n",
    "    d = datetime.strftime(datetime.strptime(\"2023-04-18\", \"%Y-%m-%d\"), \"%m/%d/%Y\")\n",
    "    yields = yield_rates.loc[yield_rates[\"Date\"]==d].values[:,1:].astype(np.float64).reshape(-1)\n",
    "    scaler = MinMaxScaler()\n",
    "    yields_normalized = scaler.fit_transform(yields.reshape(-1, 1)).flatten()\n",
    "    curve_fit, _ = calibrate_nss_ols(yield_maturities, yields_normalized)\n",
    "    volSurface['rate'] = volSurface['maturity'].apply(curve_fit) / 100\n",
    "    volSurface['IV'] = implied_volatility(volSurface['price'], volSurface['S'], volSurface['strike'], volSurface['maturity'], volSurface['rate'], 'p')\n",
    "    return volSurface\n",
    "\n",
    "def heston_daily_volSurface(underlying_ticker, date):\n",
    "    cleaned = daily_option_data(underlying_ticker, date)\n",
    "    yield_rates = pd.read_csv(\"five-year-rates.csv\")\n",
    "   \n",
    "    d = datetime.strftime(datetime.strptime(date, \"%Y-%m-%d\"), \"%m/%d/%Y\")\n",
    "    yields = yield_rates.loc[yield_rates[\"Date\"]==d].values[:,1:].astype(np.float64).reshape(-1)\n",
    "    \n",
    "    volSurface = heston_volSurface(cleaned, yields)\n",
    "    return volSurface\n",
    "\n",
    "\n",
    "def heston_parameters(VolSurface):\n",
    "\t# Extract data from dailyVolSurface DataFrame\n",
    "    _K = VolSurface['strike'].to_numpy()\n",
    "\n",
    "    _C = VolSurface['price'].to_numpy()\n",
    "    _T      = VolSurface['maturity'].to_numpy()\n",
    "    _r      = VolSurface['rate'].to_numpy()\n",
    "    _S      = VolSurface['S'].to_numpy()\n",
    "    _IV     = VolSurface['IV'].to_numpy()\n",
    "    _Weight = VolSurface['Weight'].to_numpy()\n",
    "    # Initial parameters and bounds for optimization\n",
    "    params = {\n",
    "        \"v0\": {\"x0\": np.random.uniform(1e-3, 1.2), \"lbub\": [1e-3, 1.2]},\n",
    "        \"kappa\": {\"x0\": np.random.uniform(1e-3, 10), \"lbub\": [1e-3, 10]},\n",
    "        \"theta\": {\"x0\": np.random.uniform(1e-3, 1), \"lbub\": [1e-3, 1.2]},\n",
    "        \"zeta\": {\"x0\": np.random.uniform(1e-2, 4), \"lbub\": [1e-2, 4]},\n",
    "        \"rho\": {\"x0\": np.random.uniform(-1, 1), \"lbub\": [-1, 1]}\n",
    "    }\n",
    "    x0 = [param[\"x0\"] for _, param in params.items()]\n",
    "    bnds = [param[\"lbub\"] for _, param in params.items()]\n",
    "    result = minimize(\n",
    "    SqErr, x0, args=(VolSurface,  _S, _K, _T, _r, _IV, _Weight),  tol=1e-5, method='SLSQP',\n",
    "    options={'maxiter': 80, 'ftol': 1e-5, 'disp': True},\n",
    "    bounds=bnds, jac='3-point'\n",
    "\t)\n",
    "\n",
    "    return result.x\n",
    "\n",
    "def heston_day_params(underlying_ticker, date):\n",
    "    volSurface = heston_daily_volSurface(underlying_ticker, date)\n",
    "    return heston_parameters(volSurface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import LinAlgError\n",
    "def heston_params(underlying_ticker, start_date, end_date):\n",
    "\tdates = mcal.get_calendar(\"NYSE\").valid_days(start_date=start_date, end_date=end_date)\n",
    "\t# params = [heston_day_params(underlying_ticker, date.date().strftime(\"%Y-%m-%d\")) for date in dates]\n",
    "\tdf = pd.DataFrame(columns=[\"date\", 'v0', 'kappa', 'theta', 'zeta', 'rho'])\n",
    "\tfor date in dates:\n",
    "\t\tprint(\"optimizing for\", date.date().strftime(\"%Y-%m-%d\"), f\"for {underlying_ticker}\")\n",
    "\t\ttries = 0\n",
    "\t\twhile tries < 3:\n",
    "\t\t\ttry:\n",
    "\n",
    "\t\t\t\tparams = heston_day_params(underlying_ticker, date.date().strftime(\"%Y-%m-%d\"))\n",
    "\t\t\t\trow = pd.DataFrame({\n",
    "\t\t\t\t\t\"date\": date.date().strftime(\"%Y-%m-%d\"),\n",
    "\t\t\t\t\t'v0': params[0],\n",
    "\t\t\t\t\t'kappa': params[1],\n",
    "\t\t\t\t\t'theta': params[2],\n",
    "\t\t\t\t\t'zeta': params[3],\n",
    "\t\t\t\t\t'rho': params[4]\n",
    "\t\t\t\t}, columns=[\"date\", 'v0', 'kappa', 'theta', 'zeta', 'rho'], index=[0])\n",
    "\t\t\t\tdf = pd.concat([df, row])\n",
    "\t\t\t\tbreak\n",
    "\t\t\texcept LinAlgError:\n",
    "\t\t\t\tprint(\"LinAlgError, trying again\")\n",
    "\t\t\t\ttries += 1\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# underlying_ticker = \"jnj\"\n",
    "# underlying_ticker = underlying_ticker.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = generate_option_aggs(underlying_ticker)\n",
    "# save_option_ticker(underlying_ticker, data)\n",
    "# dfs = process_multiple_days(underlying_ticker, '2023-04-11', '2024-04-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(underlying_ticker):\n",
    "\t# load_dotenv(\"/Users/brad/mlprojects/guidelight/guidelight-api/.env\")\n",
    "\tdata = generate_option_aggs(underlying_ticker)\n",
    "\tsave_option_ticker(underlying_ticker, data)\n",
    "\tdfs = process_multiple_days(underlying_ticker, '2023-04-11', '2024-04-12')\n",
    "def heston_worker(underlying_ticker):\n",
    "\tparams = heston_params(underlying_ticker, '2023-04-11', '2024-04-12')\n",
    "\tparams.to_csv(f\"heston_params/{underlying_ticker}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\ttickers = pd.read_csv(\"sp_tickers.csv\")\n",
    "\tticker_l = tickers[\"Symbol\"].values[:8]\n",
    "\t\n",
    "\t# underlying_ticker = \"jnj\"\n",
    "\t# underlying_ticker = underlying_ticker.upper()\n",
    "\tParallel(n_jobs=4)(delayed(worker)(ticker) for ticker in ticker_l)\n",
    "\tParallel(n_jobs=4)(heston_worker(ticker) for ticker in ticker_l)\n",
    "\t\n",
    "\t# print(params_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guidelight-Popvxsqg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
